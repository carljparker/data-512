{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Gender Differences when Detecting and Assessing Agression and Toxicity in Wikipeda Talk Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction and objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to explore two questions using the data in these datasets. Both questions consider possible gender differences in the way the crowdworkers interpreted the Talk Page comments. One of the alleged differencs between men and women is that woman report higher Agreeableness scores in measures of the Big Five personality traits. Another is that women have greater empathy than men. (See references below.)\n",
    "\n",
    "The difference in Agreeableness suggests the possibility that women might have--in some sense--greater reactivity to aggression than men and therefore might be more likely to flag it in the Talk Page comments. Similarly, for toxicity. Here we are operating on the assumption that Agreeableness, as a personality trait, is at odds with both aggression and toxicity.\n",
    "\n",
    "Empathy is associated with understanding the mental or emotional state of another person. If women in general have higher empathy than men, then this could manifest in multiple ways. Women might tend to assess aggression or toxicity in the Wikipedia talk comments with greater _precision_ than men. Or women might simply tend to assess higher level of aggression and toxicity than men.\n",
    "\n",
    "Note that detecting aggression/toxicity and assessing its level are somewhat distinct. For example, men and women could agree on which comments are toxic, but in general, disagree about the level of toxicity. There is also the scenario where men and women seem to disagree on which comments are toxic, but when they do agree, they rate the toxicity level similarly; admittedly, this second scenario seems less likely.\n",
    "\n",
    "-  Question 1: Do women have greater sensitivity than men _in detecting_ instances of a) aggression or b) toxicity in the Talk Page comments--related to differences in Agreeableness.\n",
    "-  Question 2: Do women have greater sensitivity than men _to the level_ of a) aggression or b) toxicity in the Talk Page comments--related to differences in empathy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weisberg, Yanna J et al. “Gender Differences in Personality across the Ten Aspects of the Big Five.” Frontiers in psychology vol. 2 178. 1 Aug. 2011, [doi:10.3389/fpsyg.2011.00178](https://dx.doi.org/10.3389%2Ffpsyg.2011.00178)\n",
    "- Christov-Moore, Leonardo et al. “Empathy: gender effects in brain and behavior.” Neuroscience and biobehavioral reviews vol. 46 Pt 4, Pt 4 (2014): 604-27, [doi:10.1016/j.neubiorev.2014.09.001](https://dx.doi.org/10.1016%2Fj.neubiorev.2014.09.001).\n",
    "- Mestre, María Vicenta, et al. “Are Women More Empathetic than Men? A Longitudinal Study in Adolescence.” The Spanish Journal of Psychology, vol. 12, no. 1, 2009, pp. 76–83., [doi:10.1017/S1138741600001499](https://doi.org/10.1017/S1138741600001499).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we load the datasets, merge them, and filter them, to enable our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect the aggression tables using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Aggression data set \n",
    "#\n",
    "aggression_annotations = pd.read_csv(\"aggression_annotations.tsv\", delimiter=\"\\t\")\n",
    "aggression_annotated_comments = pd.read_csv(\"aggression_annotated_comments.tsv\", delimiter=\"\\t\")\n",
    "aggression_worker_demographics = pd.read_csv(\"aggression_worker_demographics.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly inspect the annotation and demographic tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggression_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggression_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggression_worker_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggression_worker_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join annotation table to demographics table [Aggression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations = pd.merge( aggression_annotations, aggression_worker_demographics, left_on=\"worker_id\", right_on=\"worker_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the merge, we seem to have lost a large number (37%) of the records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records_dropped = aggression_annotations.shape[0] - joined_annotations.shape[0]\n",
    "percent_records_dropped = ( num_records_dropped / aggression_annotations.shape[0] )\n",
    "\n",
    "'Records lost in merge: {0:,} ({1:.2%})'.format( num_records_dropped, percent_records_dropped )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the records by gender [Aggression]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter on only the male workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_aggression = joined_annotations[ ( joined_annotations.gender == \"male\" ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_aggression.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_aggression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_aggression.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, filter on the female workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_aggression = joined_annotations[ ( joined_annotations.gender == \"female\" ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_aggression.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are significantly fewer female workers than male workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_male = male_joined_aggression.shape[0]\n",
    "num_female = female_joined_aggression.shape[0]\n",
    "total_workers = num_male + num_female\n",
    "'Male workers: {0:,} ({1:.2%}) | Female workers: {2:,} ({3:.2%})'.format( num_male, ( num_male / total_workers ), num_female, ( num_female / total_workers ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_aggression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_aggression.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect the toxicity tables using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Toxicity data set\n",
    "#\n",
    "toxicity_annotations = pd.read_csv(\"toxicity_annotations.tsv\", delimiter=\"\\t\")\n",
    "toxicity_annotated_comments = pd.read_csv(\"toxicity_annotated_comments.tsv\", delimiter=\"\\t\")\n",
    "toxicity_worker_demographics = pd.read_csv(\"toxicity_worker_demographics.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly inspect the annotation and demographic tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_worker_demographics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "toxicity_worker_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join annotation table to demographics table [Toxicity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations = pd.merge( toxicity_annotations, toxicity_worker_demographics, left_on=\"worker_id\", right_on=\"worker_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** In this merge, we seem to have lost fewer records (`1.34%`) than when we merged the aggression dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records_dropped = aggression_annotations.shape[0] - joined_annotations.shape[0]\n",
    "percent_records_dropped = ( num_records_dropped / aggression_annotations.shape[0] )\n",
    "\n",
    "'Records lost in merge: {0:,} ({1:.2%})'.format( num_records_dropped, percent_records_dropped )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the records by gender [Toxicity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter on only the male workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity = joined_annotations[ ( joined_annotations.gender == \"male\" ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, filter on the female workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity = joined_annotations[ ( joined_annotations.gender == \"female\" ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are significantly fewer female workers than male workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_male = male_joined_toxicity.shape[0]\n",
    "num_female = female_joined_toxicity.shape[0]\n",
    "total_workers = num_male + num_female\n",
    "'Male workers: {0:,} ({1:.2%}) | Female workers: {2:,} ({3:.2%})'.format( num_male, ( num_male / total_workers ), num_female, ( num_female / total_workers ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Do women have greater sensitivity than men _in detecting_ instances of a) aggression or b) toxicity in the Talk Page comments--related to differences in Agreeableness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of comments that were flagged as _**aggression**_ by men versus the percentage that were flagged by women?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_percent_flagged_aggression = male_joined_aggression[ 'aggression' ].sum() / male_joined_aggression[ 'aggression' ].count()\n",
    "female_percent_flagged_aggression = female_joined_aggression[ 'aggression' ].sum() / female_joined_aggression[ 'aggression' ].count()\n",
    "'Aggression :: Male: {0:.2%} vs Female: {1:.2%} | Delta: {2:.2%}'.format( male_percent_flagged_aggression, female_percent_flagged_aggression, abs( male_percent_flagged_aggression - female_percent_flagged_aggression) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the percentage of comments that were flagged as _**toxicity**_ by men versus the percentage that were flagged by women?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_percent_flagged_toxicity = male_joined_toxicity[ 'toxicity' ].sum() / male_joined_toxicity[ 'toxicity' ].count()\n",
    "female_percent_flagged_toxicity = female_joined_toxicity[ 'toxicity' ].sum() / female_joined_toxicity[ 'toxicity' ].count()\n",
    "'Toxicity :: Male: {0:.2%} vs Female: {1:.2%} | Delta: {2:.2%}'.format( male_percent_flagged_toxicity, female_percent_flagged_toxicity, abs( male_percent_flagged_toxicity - female_percent_flagged_toxicity) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(1, 2, figsize=(12, 5))\n",
    "df_aggression_plot_data = pd.DataFrame({'Gender':['Male', 'Female'], 'Percentage':[male_percent_flagged_aggression * 100, female_percent_flagged_aggression * 100]})\n",
    "df_aggression_plot_data.plot.bar(x='Gender', y='Percentage', rot=0, title='Percent of annotations flagged as aggressive', ax = axarr[0])\n",
    "\n",
    "df_toxicity_plot_data = pd.DataFrame({'Gender':['Male', 'Female'], 'Percentage':[male_percent_flagged_toxicity * 100, female_percent_flagged_toxicity * 100]})\n",
    "df_toxicity_plot_data.plot.bar(x='Gender', y='Percentage', rot=0, title='Percent of annotations flagged as toxic', ax = axarr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Do women have greater sensitivity than men _to the level_ of a) aggression or b) toxicity in the Talk Page comments--related to differences in empathy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare male and female scores (aggression and toxcity) to see if males and females differ in the way that they assess levels of these attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create histograms to compare male and female aggression scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure_size = [ 18, 5 ]\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "male_joined_aggression[ 'aggression_score' ].hist( figsize = figure_size )\n",
    "plt.ylabel(\"Number of ratings\",fontsize=12)\n",
    "plt.xlabel(\"Aggression score\",fontsize=12)\n",
    "plt.title('Males: aggression score', fontsize=15)\n",
    "plt.grid(None)\n",
    "#\n",
    "# Summary statistics: mean and std\n",
    "#\n",
    "mean_male = male_joined_aggression[ 'aggression_score' ].mean()\n",
    "std_male = male_joined_aggression[ 'aggression_score' ].std()\n",
    "s = \"Mean: {0:.3}\\nStd: {1:.3}\".format( mean_male, std_male)\n",
    "plt.text(1.5, 300000, s )\n",
    "#\n",
    "# Indicate mean and std with vertical lines\n",
    "#\n",
    "plt.axvline(x=mean_male, color='red')\n",
    "plt.axvline(x=mean_male + std_male, color='green')\n",
    "plt.axvline(x=mean_male - std_male, color='green')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "female_joined_aggression[ 'aggression_score' ].hist( figsize = figure_size )\n",
    "plt.ylabel(\"Number of ratings\",fontsize=12)\n",
    "plt.xlabel(\"Aggression score\",fontsize=12)\n",
    "plt.title('Females: aggression score', fontsize=15)\n",
    "plt.grid(None)\n",
    "#\n",
    "# Summary statistics: mean and std\n",
    "#\n",
    "mean_female = female_joined_aggression[ 'aggression_score' ].mean()\n",
    "std_female = female_joined_aggression[ 'aggression_score' ].std()\n",
    "s = \"Mean: {0:.3}\\nStd: {1:.3}\".format( mean_female, std_female)\n",
    "plt.text(1.5, 200000, s )\n",
    "#\n",
    "# Indicate mean and std with vertical lines\n",
    "#\n",
    "plt.axvline(x=mean_female, color='red')\n",
    "plt.axvline(x=mean_female + std_female, color='green')\n",
    "plt.axvline(x=mean_female - std_female, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We _aren't_ seeing much difference here. The means differ by `0.069` units and the standard deviations differ by `0.002`. Women appear to rate aggressiveness at `-3` more than `-2` and men the reverse, but without further analysis, I'm not sure how much we can make of that. Otherwise, even the shapes of the histograms are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create histograms to compare male and female toxicity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure_size = [ 18, 5 ]\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "male_joined_toxicity[ 'toxicity_score' ].hist( figsize = figure_size )\n",
    "plt.ylabel(\"Number of ratings\",fontsize=12)\n",
    "plt.xlabel(\"Toxicity score\",fontsize=12)\n",
    "plt.title('Males: toxicity score', fontsize=15)\n",
    "plt.grid(None)\n",
    "#\n",
    "# Summary statistics: mean and std\n",
    "#\n",
    "mean_male = male_joined_toxicity[ 'toxicity_score' ].mean()\n",
    "std_male = male_joined_toxicity[ 'toxicity_score' ].std()\n",
    "s = \"Mean: {0:.3}\\nStd: {1:.3}\".format( mean_male, std_male)\n",
    "plt.text(1.5, 375000, s )\n",
    "#\n",
    "# Indicate mean and std with vertical lines\n",
    "#\n",
    "plt.axvline(x=mean_male, color='red')\n",
    "plt.axvline(x=mean_male + std_male, color='green')\n",
    "plt.axvline(x=mean_male - std_male, color='green')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "female_joined_toxicity[ 'toxicity_score' ].hist( figsize = figure_size )\n",
    "plt.ylabel(\"Number of ratings\",fontsize=12)\n",
    "plt.xlabel(\"Toxicity score\",fontsize=12)\n",
    "plt.title('Females: toxicity score', fontsize=15)\n",
    "plt.grid(None)\n",
    "#\n",
    "# Summary statistics: mean and std\n",
    "#\n",
    "mean_female = female_joined_toxicity[ 'toxicity_score' ].mean()\n",
    "std_female = female_joined_toxicity[ 'toxicity_score' ].std()\n",
    "s = \"Mean: {0:.3}\\nStd: {1:.3}\".format( mean_female, std_female)\n",
    "plt.text(1.5, 200000, s )\n",
    "#`\n",
    "# Indicate mean and std with vertical lines\n",
    "#\n",
    "plt.axvline(x=mean_female, color='red')\n",
    "plt.axvline(x=mean_female + std_female, color='green')\n",
    "plt.axvline(x=mean_female - std_female, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case (toxicity), the means differ by `0.048` and the standard deviations differ by `0.02`. Also, these two histograms have similar shapes. In fact, they look so similar that if the y-axes didn't have different values, I would suspect that they were the same. \n",
    "\n",
    "Based on these two sets of histograms, I can't conclude that there is a difference in the way that men and women evaluate aggression or toxicity in the Wikipedia Talk Page comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Male and female comparison of demographic characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section compares age and education demographics for the male and female workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on ordering the levels on the x-axis of the histograms for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the education and age histograms, I needed to engage in some _gymnastics_ in order for the values on the x-axis to come out in a reasonable order. The process involves creating a mapping between the levels of the columns and an ordered set of non-negative integers. Then we use that mapping to create an additional column in the dataframe that we can use to sort the levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_groups = ['none', 'some', 'hs', 'bachelors', 'masters', 'professional', 'doctorate']\n",
    "mapping = {edu_map: i for i, edu_map in enumerate(edu_groups)}\n",
    "#\n",
    "# Here is what the mapping looks like.\n",
    "#\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join annotation table to demographics table [Toxicity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations = pd.merge( toxicity_annotations, toxicity_worker_demographics, left_on=\"worker_id\", right_on=\"worker_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the merge, we seem to have lost a large number (37%) of the records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records_dropped = aggression_annotations.shape[0] - joined_annotations.shape[0]\n",
    "percent_records_dropped = ( num_records_dropped / aggression_annotations.shape[0] )\n",
    "\n",
    "'Records lost in merge: {0:,} ({1:.2%})'.format( num_records_dropped, percent_records_dropped )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect the toxicity tables using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the records by gender [Toxicity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter on only the male workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity = joined_annotations[ ( joined_annotations.gender == \"male\" ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_joined_toxicity.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, filter on the female workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity = joined_annotations[ ( joined_annotations.gender == \"female\" ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are significantly fewer female workers than male workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_male = male_joined_toxicity.shape[0]\n",
    "num_female = female_joined_toxicity.shape[0]\n",
    "total_workers = num_male + num_female\n",
    "'Male workers: {0:,} ({1:.2%}) | Female workers: {2:,} ({3:.2%})'.format( num_male, ( num_male / total_workers ), num_female, ( num_female / total_workers ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_joined_toxicity.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_map = male_joined_annotations['education'].map(mapping)\n",
    "#\n",
    "# Here is the result of running the mapping on the 'education' column\n",
    "#\n",
    "edu_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The series has the name of the column that we mapped from. In this case, 'education'.\n",
    "# Need to change that so we don't end up with two columns named 'education' in the dataframe.\n",
    "#\n",
    "edu_map = edu_map.rename( 'edu_sort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat( [male_joined_annotations, edu_map ], axis = 1).sort_values( by = 'edu_sort')[ 'education' ].hist()\n",
    "plt.ylabel(\"Number of ratings\",fontsize=12)\n",
    "plt.xlabel(\"Education\",fontsize=12)\n",
    "plt.title('Males: education', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following code to enable ordering the columns of the histogram in a reasonable way. For more detail, see the proceeding discussion regarding the 'education' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_groups = ['Under 18', '18-30', '30-45', '45-60', 'Over 60']\n",
    "mapping = {age_map: i for i, age_map in enumerate( age_groups )}\n",
    "age_map = male_joined_annotations[ 'age_group' ].map( mapping )\n",
    "age_map = age_map.rename( 'age_sort' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat( [male_joined_annotations, age_map ], axis = 1).sort_values( by = 'age_sort')[ 'age_group' ].hist()\n",
    "plt.ylabel(\"Number of ratings\",fontsize=12)\n",
    "plt.xlabel(\"Age\",fontsize=12)\n",
    "plt.title('Males: age', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you start asking question.\n",
    "\n",
    "I can calculate the average score per worker - that worker's \"toxicity bias\". Is this different for different age groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_worker_aggression = joined_annotations.groupby(\"worker_id\")[\"aggression_score\"].mean()\n",
    "aggression_worker_demographics = aggression_worker_demographics.join( avg_worker_aggression )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look at our newly augmented table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggression_worker_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute an average toxicity statistic for each group..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggression_worker_demographics.groupby(\"age_group\").aggression_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"toxicity bias\" does vary by group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even plot the distribution of personal biases in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.set_title(\"Distribution of worker's mean toxicity rating, by age group\")\n",
    "sns.violinplot( x=\"aggression_score\", y=\"age_group\", data=aggression_worker_demographics, ax=ax )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very basic bias question: is the set of annotation workers gender-balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = aggression_worker_demographics.groupby(\"gender\").worker_id.count()\n",
    "foo/foo.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is **no** - 64.8% of annotators are male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the set of _annotations_ gender-balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = joined_annotations.groupby(\"gender\").rev_id.count()\n",
    "foo/foo.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also no, about the same fraction - 64.8% of annotations - were made by male annotators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
